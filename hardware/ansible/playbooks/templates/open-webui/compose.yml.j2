---
services:
  # Currently, we do not expose ollama-no-gpu. We could consider doing so
  # later via tailscale/caddy.
  ollama-no-gpu:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    volumes:
      - /mnt/data/ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - CUDA_VISIBLE_DEVICES=
    network_mode: service:tailscale

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    restart: unless-stopped
    volumes:
      - /mnt/data/open-webui-data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://localhost:11434
      - HOST=0.0.0.0
      - PORT=8080
    depends_on:
      - ollama-no-gpu
    network_mode: service:tailscale

  # TODO: Add bedrock-gateway

  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    restart: unless-stopped
    hostname: open-webui
    volumes:
      - /mnt/data/tailscale-data:/var/lib/tailscale
      - tailscale-tmp:/tmp
    environment:
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_HOSTNAME={{ service_name }}
      - TS_USERSPACE=true
      - TS_EXTRA_ARGS=--advertise-tags={{ tailscale_advertise_tags_arg }}
      - TS_SOCKET=/tmp/tailscaled.sock
    cap_add:
      - NET_ADMIN
    env_file:
      - .env.tailscale
    ports:
      - "11434:11434"
      - "{{ container_port }}:{{ container_port }}"
      - "443:443"
    networks:
      - internal

  caddy:
    image: caddy:2-alpine
    container_name: caddy
    restart: unless-stopped
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - /mnt/data/caddy-data:/data
      - caddy-config:/config
      - tailscale-tmp:/var/run/tailscale
    depends_on:
      - open-webui
    network_mode: service:tailscale

networks:
  internal:
    driver: bridge

volumes:
  caddy-config:
  tailscale-tmp:
