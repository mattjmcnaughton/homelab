---
services:
  # For now, we do not expose ollama-no-gpu. But we could consider doing so
  # later via tailscale/caddy.
  ollama-no-gpu:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - CUDA_VISIBLE_DEVICES=
    network_mode: service:tailscale

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    restart: unless-stopped
    volumes:
      - open-webui-data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://localhost:11434
      - HOST=0.0.0.0
      - PORT=8080
    depends_on:
      - ollama-no-gpu
    network_mode: service:tailscale

  # TODO: Add bedrock-gateway

  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    restart: unless-stopped
    hostname: open-webui
    volumes:
      - tailscale-data:/var/lib/tailscale
      - tailscale-tmp:/tmp
    environment:
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_HOSTNAME=open-webui
      - TS_USERSPACE=true
      - TS_EXTRA_ARGS=--advertise-tags=tag:homelab-auto-provision
      - TS_SOCKET=/tmp/tailscaled.sock
      - TS_PERMIT_CERT_UID=caddy
    cap_add:
      - NET_ADMIN
    env_file:
      - .env.tailscale
    ports:
      - "11434:11434"
      - "8080:8080"
      - "443:443"
    networks:
      - internal

  caddy:
    image: caddy:2-alpine
    container_name: caddy
    restart: unless-stopped
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy-data:/data
      - caddy-config:/config
      - tailscale-tmp:/var/run/tailscale
    depends_on:
      - open-webui
    network_mode: service:tailscale

networks:
  internal:
    driver: bridge

volumes:
  ollama-data:
  open-webui-data:
  caddy-data:
  caddy-config:
  tailscale-data:
  tailscale-tmp:
